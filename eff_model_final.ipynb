{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24221,"status":"ok","timestamp":1622733586564,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"3-m8Nrpik6tm","outputId":"a6b676d3-691a-41ca-db3a-708f69359974"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1000,"status":"ok","timestamp":1622734594323,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"yOm9qQDEg_J6","outputId":"bc893e9d-39ba-422b-f08e-538d27950e29"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/data_project3/train\n"]}],"source":["#cd \"/content/drive/MyDrive/data_project3/train\""]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":744633,"status":"ok","timestamp":1622735345711,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"2Akrt5kNg_Xw","outputId":"9be36a77-50b5-43d1-9b7b-81ee444682bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["replace 0.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"]}],"source":["#!unzip -qq \"/content/drive/MyDrive/data_project3/train/train.zip\""]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-05-14T11:09:13.550573Z","iopub.status.busy":"2021-05-14T11:09:13.549468Z","iopub.status.idle":"2021-05-14T11:09:14.598712Z","shell.execute_reply":"2021-05-14T11:09:14.599212Z"},"executionInfo":{"elapsed":3394,"status":"ok","timestamp":1622733589955,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"broke-calendar","outputId":"f1e599ed-18be-4693-d4c0-b6f5dbcad5c3","papermill":{"duration":1.072343,"end_time":"2021-05-14T11:09:14.599519","exception":false,"start_time":"2021-05-14T11:09:13.527176","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['.config', 'drive', 'sample_data']\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import numpy as np\n","import json\n","from torch.optim import lr_scheduler\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import time\n","import datetime\n","import copy\n","import random\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","import torch\n","print(os.listdir())\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":624,"status":"ok","timestamp":1622733620880,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"-2G_kVJykyHj","outputId":"d516bd65-c90f-4bb3-b45e-b5af3c4e3437"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/data_project3'"]},"execution_count":3,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import os\n","os.chdir('/content/drive/MyDrive/data_project3')\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69fv4lxiXBuR"},"outputs":[],"source":["#!git clone https://github.com/lukemelas/EfficientNet-PyTorch.git"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622733622415,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"8gRyckENY47b","outputId":"5e1176b1-674d-4d22-e29b-76593fd2de99"},"outputs":[{"name":"stdout","output_type":"stream","text":["['.git', '.github', '.gitignore', 'LICENSE', 'README.md', 'efficientnet_pytorch', 'examples', 'hubconf.py', 'setup.py', 'sotabench.py', 'sotabench_setup.sh', 'tests', 'tf_to_pytorch', 'efficientnet_pytorch.egg-info']\n"]}],"source":["os.chdir('./EfficientNet-PyTorch')\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1HWmx4gYYpt"},"outputs":[],"source":["#!pip install -e ."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":383,"status":"ok","timestamp":1622733627608,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"CFUpS7SXTFsa"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/data_project3/EfficientNet-PyTorch')"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":501,"status":"ok","timestamp":1622735921303,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"KG_Wgtg_Xj6J"},"outputs":[],"source":["from efficientnet_pytorch import EfficientNet, get_model_params\n","model = EfficientNet.from_name('efficientnet-b4')\n","#print(model)"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":750,"status":"ok","timestamp":1622735924058,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"_VWT0lKEjNFi"},"outputs":[],"source":["override={'num_classes':12, 'image_size': 5000}\n","blocks_args, global_params =get_model_params('efficientnet-b4', override)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1622735926675,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"LLcIrKYCn1NS","outputId":"a0981b3e-4723-43aa-fb4a-189f6fff3d6f"},"outputs":[{"data":{"text/plain":["[BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=2, kernel_size=3, stride=[2], expand_ratio=6, input_filters=16, output_filters=24, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=2, kernel_size=5, stride=[2], expand_ratio=6, input_filters=24, output_filters=40, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=3, kernel_size=3, stride=[2], expand_ratio=6, input_filters=40, output_filters=80, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=3, kernel_size=5, stride=[1], expand_ratio=6, input_filters=80, output_filters=112, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=4, kernel_size=5, stride=[2], expand_ratio=6, input_filters=112, output_filters=192, se_ratio=0.25, id_skip=True),\n"," BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=6, input_filters=192, output_filters=320, se_ratio=0.25, id_skip=True)]"]},"execution_count":42,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["blocks_args"]},{"cell_type":"markdown","metadata":{"id":"m7vedTABfzR_"},"source":["### Efficient Model"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1622735938144,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"Mubxq-aFa0TP"},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","class SwishImplementation(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, i):\n","        result = i * torch.sigmoid(i)\n","        ctx.save_for_backward(i)\n","        return result\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        i = ctx.saved_tensors[0]\n","        sigmoid_i = torch.sigmoid(i)\n","        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n","\n","\n","class MemoryEfficientSwish(nn.Module):\n","    def forward(self, x):\n","        return SwishImplementation.apply(x)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622735938451,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"XV0FayAccRlJ"},"outputs":[],"source":["import re\n","import math\n","import collections\n","from functools import partial\n","\n","\n","def get_same_padding_conv1d(image_size=None):\n","    \"\"\"Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n","       Static padding is necessary for ONNX exporting of models.\n","    Args:\n","        image_size (int or tuple): Size of the image.\n","    Returns:\n","        Conv2dDynamicSamePadding or Conv2dStaticSamePadding.\n","    \"\"\"\n","    return partial(Conv1dStaticSamePadding, image_size=image_size)\n","\n","class Conv1dStaticSamePadding(nn.Conv1d):\n","    \"\"\"2D Convolutions like TensorFlow's 'SAME' mode, with the given input image size.\n","       The padding mudule is calculated in construction function, then used in forward.\n","    \"\"\"\n","\n","    # With the same calculation as Conv2dDynamicSamePadding\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, image_size=None, **kwargs):\n","        super().__init__(in_channels, out_channels, kernel_size, stride, **kwargs)\n","        self.stride = stride if isinstance(stride, int) else stride[0]\n","        # Calculate padding based on image size and save it\n","        assert image_size is not None\n","        ih = image_size if isinstance(image_size, int) else image_size[0]\n","        kh = self.weight.size()[-1]\n","        sh = self.stride\n","        oh = math.ceil(ih / sh)\n","        self.pad_h = max((oh - 1) * self.stride + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n","\n","    def forward(self, x):\n","        if self.pad_h > 0:\n","          x = F.pad(x,(self.pad_h // 2, self.pad_h - self.pad_h // 2))\n","        x = F.conv1d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n","        return x\n","\n","def calculate_output_image_size(input_image_size, stride):\n","    \"\"\"Calculates the output image size when using Conv2dSamePadding with a stride.\n","       Necessary for static padding. Thanks to mannatsingh for pointing this out.\n","    Args:\n","        input_image_size (int, tuple or list): Size of input image.\n","        stride (int, tuple or list): Conv2d operation's stride.\n","    Returns:\n","        output_image_size: A list [H,W].\n","    \"\"\"\n","    if input_image_size is None:\n","        return None\n","    image_height =input_image_size\n","    stride = stride if isinstance(stride, int) else stride[0]\n","    image_height = int(math.ceil(image_height / stride))\n","    return image_height\n","\n","def drop_connect(inputs, p, training):\n","    \"\"\"Drop connect.\n","    Args:\n","        input (tensor: BCWH): Input of this structure.\n","        p (float: 0.0~1.0): Probability of drop connection.\n","        training (bool): The running mode.\n","    Returns:\n","        output: Output after drop connection.\n","    \"\"\"\n","    assert 0 <= p <= 1, 'p must be in range of [0,1]'\n","\n","    if not training:\n","        return inputs\n","\n","    batch_size = inputs.shape[0]\n","    keep_prob = 1 - p\n","\n","    # generate binary_tensor mask according to probability (p for 0, 1-p for 1)\n","    random_tensor = keep_prob\n","    random_tensor += torch.rand([batch_size, 1, 1], dtype=inputs.dtype, device=inputs.device)\n","    binary_tensor = torch.floor(random_tensor)\n","\n","    output = inputs / keep_prob * binary_tensor\n","    return output\n","\n","def round_filters(filters, global_params):\n","    \"\"\"Calculate and round number of filters based on width multiplier.\n","       Use width_coefficient, depth_divisor and min_depth of global_params.\n","    Args:\n","        filters (int): Filters number to be calculated.\n","        global_params (namedtuple): Global params of the model.\n","    Returns:\n","        new_filters: New filters number after calculating.\n","    \"\"\"\n","    multiplier = global_params.width_coefficient\n","    if not multiplier:\n","        return filters\n","    # TODO: modify the params names.\n","    #       maybe the names (width_divisor,min_width)\n","    #       are more suitable than (depth_divisor,min_depth).\n","    divisor = global_params.depth_divisor\n","    min_depth = global_params.min_depth\n","    filters *= multiplier\n","    min_depth = min_depth or divisor  # pay attention to this line when using min_depth\n","    # follow the formula transferred from official TensorFlow implementation\n","    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n","    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n","        new_filters += divisor\n","    return int(new_filters)\n","\n","def round_repeats(repeats, global_params):\n","    \"\"\"Calculate module's repeat number of a block based on depth multiplier.\n","       Use depth_coefficient of global_params.\n","    Args:\n","        repeats (int): num_repeat to be calculated.\n","        global_params (namedtuple): Global params of the model.\n","    Returns:\n","        new repeat: New repeat number after calculating.\n","    \"\"\"\n","    multiplier = global_params.depth_coefficient\n","    if not multiplier:\n","        return repeats\n","    # follow the formula transferred from official TensorFlow implementation\n","    return int(math.ceil(multiplier * repeats))"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622735938452,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"D7ezV1VGbEbM"},"outputs":[],"source":["class MBConvBlock(nn.Module):\n","    \"\"\"Mobile Inverted Residual Bottleneck Block.\n","    Args:\n","        block_args (namedtuple): BlockArgs, defined in utils.py.\n","        global_params (namedtuple): GlobalParam, defined in utils.py.\n","        image_size (tuple or list): [image_height, image_width].\n","    References:\n","        [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)\n","        [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)\n","        [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)\n","    \"\"\"\n","\n","    def __init__(self, block_args, global_params, image_size=None):\n","        super().__init__()\n","        self._block_args = block_args\n","        self._bn_mom = 1 - global_params.batch_norm_momentum  # pytorch's difference from tensorflow\n","        self._bn_eps = global_params.batch_norm_epsilon\n","        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n","        self.id_skip = block_args.id_skip  # whether to use skip connection and drop connect\n","\n","        # Expansion phase (Inverted Bottleneck)\n","        inp = self._block_args.input_filters  # number of input channels\n","        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n","        if self._block_args.expand_ratio != 1:\n","            Conv1d = get_same_padding_conv1d(image_size=image_size)\n","            self._expand_conv = Conv1d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n","            self._bn0 = nn.BatchNorm1d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n","            # image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size\n","\n","        # Depthwise convolution phase\n","        k = self._block_args.kernel_size\n","        s = self._block_args.stride\n","        Conv1d = get_same_padding_conv1d(image_size=image_size)\n","        self._depthwise_conv = Conv1d(\n","            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n","            kernel_size=k, stride=s, bias=False)\n","        self._bn1 = nn.BatchNorm1d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n","        image_size = calculate_output_image_size(image_size, s)\n","\n","        # Squeeze and Excitation layer, if desired\n","        if self.has_se:\n","            Conv1d = get_same_padding_conv1d(image_size=(1, 1))\n","            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n","            self._se_reduce = Conv1d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n","            self._se_expand = Conv1d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n","\n","        # Pointwise convolution phase\n","        final_oup = self._block_args.output_filters\n","        Conv1d = get_same_padding_conv1d(image_size=image_size)\n","        self._project_conv = Conv1d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n","        self._bn2 = nn.BatchNorm1d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n","        self._swish = MemoryEfficientSwish()\n","\n","    def forward(self, inputs, drop_connect_rate=None):\n","        \"\"\"MBConvBlock's forward function.\n","        Args:\n","            inputs (tensor): Input tensor.\n","            drop_connect_rate (bool): Drop connect rate (float, between 0 and 1).\n","        Returns:\n","            Output of this block after processing.\n","        \"\"\"\n","\n","        # Expansion and Depthwise Convolution\n","        x = inputs\n","        if self._block_args.expand_ratio != 1:\n","            x = self._expand_conv(inputs)\n","            x = self._bn0(x)\n","            x = self._swish(x)\n","\n","        x = self._depthwise_conv(x)\n","        x = self._bn1(x)\n","        x = self._swish(x)\n","\n","        # Squeeze and Excitation\n","        if self.has_se:\n","            x_squeezed = F.adaptive_avg_pool1d(x, 1)\n","            x_squeezed = self._se_reduce(x_squeezed)\n","            x_squeezed = self._swish(x_squeezed)\n","            x_squeezed = self._se_expand(x_squeezed)\n","            x = torch.sigmoid(x_squeezed) * x\n","\n","        # Pointwise Convolution\n","        x = self._project_conv(x)\n","        x = self._bn2(x)\n","\n","        # Skip connection and drop connect\n","        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n","        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n","            # The combination of skip connection and drop connect brings about stochastic depth.\n","            if drop_connect_rate:\n","                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n","            x = x + inputs  # skip connection\n","        return x\n"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1622735938757,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"jCf9jS8HgsF_"},"outputs":[],"source":["class EfficientNet(nn.Module):\n","    \"\"\"EfficientNet model.\n","       Most easily loaded with the .from_name or .from_pretrained methods.\n","    Args:\n","        blocks_args (list[namedtuple]): A list of BlockArgs to construct blocks.\n","        global_params (namedtuple): A set of GlobalParams shared between blocks.\n","    References:\n","        [1] https://arxiv.org/abs/1905.11946 (EfficientNet)\n","    Example:\n","        >>> import torch\n","        >>> from efficientnet.model import EfficientNet\n","        >>> inputs = torch.rand(1, 3, 224, 224)\n","        >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n","        >>> model.eval()\n","        >>> outputs = model(inputs)\n","    \"\"\"\n","\n","    def __init__(self, blocks_args=None, global_params=None):\n","        super(EfficientNet, self).__init__()\n","        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n","        assert len(blocks_args) > 0, 'block args must be greater than 0'\n","        self._global_params = global_params\n","        self._blocks_args = blocks_args\n","\n","        # Batch norm parameters\n","        bn_mom = 1 - self._global_params.batch_norm_momentum\n","        bn_eps = self._global_params.batch_norm_epsilon\n","\n","        # Get stem static or dynamic convolution depending on image size\n","        image_size = global_params.image_size\n","        Conv1d = get_same_padding_conv1d(image_size=image_size)\n","\n","        # Stem\n","        in_channels = 2  # rgb\n","        out_channels = round_filters(32, self._global_params)  # number of output channels\n","        self._conv_stem = Conv1d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n","        self._bn0 = nn.BatchNorm1d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n","        image_size = calculate_output_image_size(image_size, 2)\n","\n","        # Build blocks\n","        self._blocks = nn.ModuleList([])\n","        for block_args in self._blocks_args:\n","\n","            # Update block input and output filters based on depth multiplier.\n","            block_args = block_args._replace(\n","                input_filters=round_filters(block_args.input_filters, self._global_params),\n","                output_filters=round_filters(block_args.output_filters, self._global_params),\n","                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n","            )\n","\n","            # The first block needs to take care of stride and filter size increase.\n","            self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n","            image_size = calculate_output_image_size(image_size, block_args.stride)\n","            if block_args.num_repeat > 1:  # modify block_args to keep same output size\n","                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n","            for _ in range(block_args.num_repeat - 1):\n","                self._blocks.append(MBConvBlock(block_args, self._global_params, image_size=image_size))\n","                # image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1\n","\n","        # Head\n","        in_channels = block_args.output_filters  # output of final block\n","        out_channels = round_filters(1280, self._global_params)\n","        Conv1d = get_same_padding_conv1d(image_size=image_size)\n","        self._conv_head = Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n","        self._bn1 = nn.BatchNorm1d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n","\n","        # Final linear layer\n","        self._avg_pooling = nn.AdaptiveAvgPool1d(7)\n","        if self._global_params.include_top:\n","            self._dropout = nn.Dropout(self._global_params.dropout_rate)\n","            self._fc = nn.Linear(out_channels*7, self._global_params.num_classes)\n","\n","        # set activation to memory efficient \n","        # by default\n","        self._swish = MemoryEfficientSwish()\n","        # set activation to memory efficient swish by default\n","    def set_swish(self, memory_efficient=True):\n","        \"\"\"Sets swish function as memory efficient (for training) or standard (for export).\n","        Args:\n","            memory_efficient (bool): Whether to use memory-efficient version of swish.\n","        \"\"\"\n","        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n","        for block in self._blocks:\n","            block.set_swish(memory_efficient)\n","\n","    def extract_endpoints(self, inputs):\n","        \"\"\"Use convolution layer to extract features\n","        from reduction levels i in [1, 2, 3, 4, 5].\n","        Args:\n","            inputs (tensor): Input tensor.\n","        Returns:\n","            Dictionary of last intermediate features\n","            with reduction levels i in [1, 2, 3, 4, 5].\n","            Example:\n","                >>> import torch\n","                >>> from efficientnet.model import EfficientNet\n","                >>> inputs = torch.rand(1, 3, 224, 224)\n","                >>> model = EfficientNet.from_pretrained('efficientnet-b0')\n","                >>> endpoints = model.extract_endpoints(inputs)\n","                >>> print(endpoints['reduction_1'].shape)  # torch.Size([1, 16, 112, 112])\n","                >>> print(endpoints['reduction_2'].shape)  # torch.Size([1, 24, 56, 56])\n","                >>> print(endpoints['reduction_3'].shape)  # torch.Size([1, 40, 28, 28])\n","                >>> print(endpoints['reduction_4'].shape)  # torch.Size([1, 112, 14, 14])\n","                >>> print(endpoints['reduction_5'].shape)  # torch.Size([1, 320, 7, 7])\n","                >>> print(endpoints['reduction_6'].shape)  # torch.Size([1, 1280, 7, 7])\n","        \"\"\"\n","        endpoints = dict()\n","\n","        # Stem\n","        x = self._swish(self._bn0(self._conv_stem(inputs)))\n","        prev_x = x\n","\n","        # Blocks\n","        for idx, block in enumerate(self._blocks):\n","            drop_connect_rate = self._global_params.drop_connect_rate\n","            if drop_connect_rate:\n","                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n","            x = block(x, drop_connect_rate=drop_connect_rate)\n","            if prev_x.size(2) > x.size(2):\n","                endpoints['reduction_{}'.format(len(endpoints) + 1)] = prev_x\n","            elif idx == len(self._blocks) - 1:\n","                endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n","            prev_x = x\n","\n","        # Head\n","        x = self._swish(self._bn1(self._conv_head(x)))\n","        endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n","\n","        return endpoints\n","\n","    def extract_features(self, inputs):\n","        \"\"\"use convolution layer to extract feature .\n","        Args:\n","            inputs (tensor): Input tensor.\n","        Returns:\n","            Output of the final convolution\n","            layer in the efficientnet model.\n","        \"\"\"\n","        # Stem\n","        x = self._swish(self._bn0(self._conv_stem(inputs)))\n","\n","        # Blocks\n","        for idx, block in enumerate(self._blocks):\n","            drop_connect_rate = self._global_params.drop_connect_rate\n","            if drop_connect_rate:\n","                drop_connect_rate *= float(idx) / len(self._blocks)  # scale drop connect_rate\n","            x = block(x, drop_connect_rate=drop_connect_rate)\n","\n","        # Head\n","        x = self._swish(self._bn1(self._conv_head(x)))\n","\n","        return x\n","\n","    def forward(self, inputs):\n","        \"\"\"EfficientNet's forward function.\n","           Calls extract_features to extract features, applies final linear layer, and returns logits.\n","        Args:\n","            inputs (tensor): Input tensor.\n","        Returns:\n","            Output of this model after processing.\n","        \"\"\"\n","        # Convolution layers\n","        x = self.extract_features(inputs)\n","        # Pooling and final linear layer\n","        x = self._avg_pooling(x)\n","        if self._global_params.include_top:\n","            x = x.flatten(start_dim=1)\n","            x = self._dropout(x)\n","            x = self._fc(x)\n","        return x"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":489,"status":"ok","timestamp":1622735939244,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"t_u_a5qKk6Ty"},"outputs":[],"source":["model=EfficientNet(blocks_args, global_params)"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622735939245,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"5rETlXDT7Ouj"},"outputs":[],"source":["#print(model)"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2806,"status":"ok","timestamp":1622735942049,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"RSh0l_19nbKz","outputId":"8a301a1c-c220-4ef4-9cfb-32b2f05492df"},"outputs":[{"data":{"text/plain":["tensor([[ 0.2909,  0.0997,  0.2718,  0.1190, -0.1312, -0.3882, -0.1366,  0.1344,\n","         -0.3354,  0.1046, -0.4847, -0.3504],\n","        [-0.0604,  0.1397,  0.0334, -0.1573,  0.0213, -0.0520, -0.1006,  0.1978,\n","         -0.0424,  0.0417,  0.0256,  0.0061]], grad_fn=<AddmmBackward>)"]},"execution_count":49,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["input=torch.rand((2,2,5000))\n","output=model(input)\n","output"]},{"cell_type":"markdown","metadata":{"id":"rubber-missile","papermill":{"duration":0.018298,"end_time":"2021-05-14T11:09:14.681352","exception":false,"start_time":"2021-05-14T11:09:14.663054","status":"completed"},"tags":[]},"source":["### Helper functions"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:09:14.737220Z","iopub.status.busy":"2021-05-14T11:09:14.736145Z","iopub.status.idle":"2021-05-14T11:09:14.739262Z","shell.execute_reply":"2021-05-14T11:09:14.738758Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622735942050,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"engaged-rehabilitation","papermill":{"duration":0.039374,"end_time":"2021-05-14T11:09:14.739407","exception":false,"start_time":"2021-05-14T11:09:14.700033","status":"completed"},"tags":[]},"outputs":[],"source":["def extract_age(info_file):\n","    '''\n","        info file(###.txt)로부터 나이 정보를 뽑아냅니다.\n","    '''\n","    with open(info_file, 'r') as f:\n","        info = f.read()\n","        for i, line in enumerate(info.split(\"\\n\")):\n","            if line.startswith(\"#Age\"):\n","                age = float(line.split(\": \")[1].strip())\n","    return age\n","\n","def extract_sex(info_file):\n","    '''\n","        info file(###.txt)로부터 성별 정보를 뽑아냅니다.\n","    '''\n","    with open(info_file, 'r') as f:\n","            info = f.read()\n","            for i, line in enumerate(info.split(\"\\n\")):\n","                if line.startswith(\"#Sex\"):\n","                    sex = line.split(\": \")[1].strip()\n","    return sex\n","\n","\n","def extract_labels(info_file):\n","    '''\n","        info file(###.txt)로부터 label(들) 정보를 뽑아냅니다.\n","    '''\n","    with open(info_file, 'r') as f:\n","            info = f.read()\n","            for i, line in enumerate(info.split(\"\\n\")):\n","                if line.startswith(\"#Dx\"):\n","                    labels = line.split(\": \")[1].strip()\n","                    labels = labels.split()\n","    return labels\n","\n","def read_files(data_directory, is_training=True):\n","    '''\n","        data directory(train 또는 test)로부터 모든 sample들의\n","        id, age, sex, recording, labels 정보를 읽어들여\n","        (id, age, sex, recording, labels)의 list를 반환합니다.\n","        is_training=False일 경우엔 labels 정보를 읽어들이지 않습니다.\n","    '''\n","    list_id = []\n","    list_age = []\n","    list_sex = []\n","    list_recording = []\n","    list_labels = []\n","    for f in os.listdir(data_directory):\n","        root, extension = os.path.splitext(f)\n","        if not root.startswith(\".\") and extension == \".txt\":\n","            list_id.append(int(root))\n","            info_file = os.path.join(data_directory, root + \".txt\")\n","            recording_file = os.path.join(data_directory, root + \".npy\")\n","            age = extract_age(info_file)\n","            list_age.append(age)\n","            sex = extract_sex(info_file)\n","            list_sex.append(sex)\n","            with open(recording_file, 'rb') as g:\n","                recording = np.load(g)\n","                list_recording.append(recording)\n","            if is_training:\n","                labels = extract_labels(info_file)\n","                list_labels.append(labels)\n","    if is_training:\n","        return list(zip(list_id, list_age, list_sex, list_recording, list_labels))\n","    else:\n","        return list(zip(list_id, list_age, list_sex, list_recording))"]},{"cell_type":"markdown","metadata":{"id":"broad-victoria","papermill":{"duration":0.018248,"end_time":"2021-05-14T11:09:14.776399","exception":false,"start_time":"2021-05-14T11:09:14.758151","status":"completed"},"tags":[]},"source":["### PyTorch Custom Dataset\n","training sample을 batch 단위로 처리할 수 있도록 torch.uitls.data.Dataset을 이용한 custom dataset을 만들어 줍니다."]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:09:14.825991Z","iopub.status.busy":"2021-05-14T11:09:14.825210Z","iopub.status.idle":"2021-05-14T11:09:14.828411Z","shell.execute_reply":"2021-05-14T11:09:14.827925Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1622735942498,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"helpful-burlington","papermill":{"duration":0.033677,"end_time":"2021-05-14T11:09:14.828542","exception":false,"start_time":"2021-05-14T11:09:14.794865","status":"completed"},"tags":[]},"outputs":[],"source":["class Dataset_ECG(torch.utils.data.Dataset):\n","    \"\"\"\n","        Build ECG dataset\n","    \"\"\"\n","    def __init__(self, dataset, num_classes=12):\n","        \"\"\"\n","            dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n","        \"\"\"\n","        self.sample_id = []\n","        self.sample_age = []\n","        self.sample_sex = []\n","        self.sample_recording = []\n","        self.sample_labels = []\n","        self.num_samples = len(dataset)\n","        \n","        for idx in range(self.num_samples):\n","            _id, _age, _sex, _recording, _labels = dataset[idx]\n","            # model에 input으로 들어가는 data는 torch.Tensor 타입으로 변환해 줍니다.\n","            age = torch.tensor(_age//10)\n","            sex = torch.tensor([0,1]) if _sex == \"F\" else torch.tensor([1,0])\n","            recording = torch.tensor(_recording)\n","            labels = torch.tensor(np.zeros(num_classes))\n","            for label in _labels:\n","                labels[int(label)] = 1\n","\n","            self.sample_id.append(_id)\n","            self.sample_age.append(age)\n","            self.sample_sex.append(sex)\n","            self.sample_recording.append(recording)\n","            self.sample_labels.append(labels)\n","\n","        print(f'Loaded {self.num_samples} samples...')\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"id\": self.sample_id[idx],\n","            \"age\": self.sample_age[idx],\n","            \"sex\": self.sample_sex[idx],\n","            \"recording\": self.sample_recording[idx],\n","            \"labels\": self.sample_labels[idx]\n","        }"]},{"cell_type":"markdown","metadata":{"id":"delayed-nowhere","papermill":{"duration":0.017992,"end_time":"2021-05-14T11:09:14.866131","exception":false,"start_time":"2021-05-14T11:09:14.848139","status":"completed"},"tags":[]},"source":["### PyTorch CNN model\n","간단한 CNN model을 만들어 보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"royal-debut","papermill":{"duration":0.018337,"end_time":"2021-05-14T11:09:14.956898","exception":false,"start_time":"2021-05-14T11:09:14.938561","status":"completed"},"tags":[]},"source":["### Setup"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1622735945046,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"Mgk5KBwy5D8Y"},"outputs":[],"source":["#print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/data_project3')"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2021-05-14T11:13:58.013526Z","iopub.status.busy":"2021-05-14T11:13:58.012808Z","iopub.status.idle":"2021-05-14T11:13:58.016036Z","shell.execute_reply":"2021-05-14T11:13:58.016610Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1622735947418,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"black-socket","outputId":"1331cda1-a920-43d6-9b3a-7c868c53cea6","papermill":{"duration":0.027534,"end_time":"2021-05-14T11:13:58.016818","exception":false,"start_time":"2021-05-14T11:13:57.989284","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# cuda gpu를 사용할 수 있을 경우 사용합니다.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:13:59.397195Z","iopub.status.busy":"2021-05-14T11:13:59.396186Z","iopub.status.idle":"2021-05-14T11:13:59.400441Z","shell.execute_reply":"2021-05-14T11:13:59.399460Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1622735949179,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"handmade-termination","papermill":{"duration":0.029476,"end_time":"2021-05-14T11:13:59.400630","exception":false,"start_time":"2021-05-14T11:13:59.371154","status":"completed"},"tags":[]},"outputs":[],"source":["# Training에 사용될 hyperparameter를 정해줍니다.\n","EPOCHS = 50\n","BATCH_SIZE = 16\n","LEARNING_RATE = 0.001"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1622735950476,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"vQaP8nSviIlP"},"outputs":[],"source":["training_dir = '/content/drive/MyDrive/data_project3/train'\n","test_dir = '/content/drive/MyDrive/data_project3/test'"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":97526,"status":"error","timestamp":1622736050293,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"TXWZke_y3Ayn","outputId":"67b30f96-ff34-4ebf-fa6b-8a9c7da7a159"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-3117c09e00aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training_dataset = torch.load('augmented_train_2.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#validation_set = torch.load('val.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-c0220d156da1>\u001b[0m in \u001b[0;36mread_files\u001b[0;34m(data_directory, is_training)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0minfo_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mrecording_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mlist_age\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0msex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_sex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-c0220d156da1>\u001b[0m in \u001b[0;36mextract_age\u001b[0;34m(info_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m         info file(###.txt)로부터 나이 정보를 뽑아냅니다.\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#training_dataset = torch.load('augmented_train_2.pt')\n","training_data = sorted(read_files(training_dir), key=lambda sample: sample[0])\n","#validation_set = torch.load('val.pt')\n","num_training = len(training_data)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2117,"status":"ok","timestamp":1622736054715,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"O0Cq-xVadskW","outputId":"2d89ec50-183b-459e-ff4b-984cc53541a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 19212 samples...\n"]}],"source":["training_dataset = Dataset_ECG(training_data, num_classes=12)"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:13:59.446509Z","iopub.status.busy":"2021-05-14T11:13:59.445851Z","iopub.status.idle":"2021-05-14T11:13:59.449248Z","shell.execute_reply":"2021-05-14T11:13:59.448702Z"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1622736056882,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"scientific-jordan","papermill":{"duration":0.0285,"end_time":"2021-05-14T11:13:59.449380","exception":false,"start_time":"2021-05-14T11:13:59.420880","status":"completed"},"tags":[]},"outputs":[],"source":["# Training dataset을 batch 단위로 읽어들일 수 있도록 DataLoader를 만들어줍니다.\n","training_loader = torch.utils.data.DataLoader(training_dataset,pin_memory=True, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"contemporary-costa","papermill":{"duration":0.020035,"end_time":"2021-05-14T11:13:59.489647","exception":false,"start_time":"2021-05-14T11:13:59.469612","status":"completed"},"tags":[]},"source":["### Training"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1622736058516,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"wFm24yGUvnC_"},"outputs":[],"source":["model = model.to(device)\n","model.to(device)\n","model.train()\n","pos=[811, 1062, 958, 3204, 574, 515, 625, 1203, 10258, 1114, 2451, 563]\n","p=[15370-x for x in pos]\n","p_w=[x/y for x, y in zip(p, pos)]\n","criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(p_w).to(device)) # for multi-label classification\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6160,"status":"ok","timestamp":1622733702720,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"UWA6MeeJg1GR","outputId":"e0fc841d-c562-42f0-8c68-ac65408cc8b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git\n","  Cloning https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git to /tmp/pip-req-build-yacw49k4\n","  Running command git clone -q https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git /tmp/pip-req-build-yacw49k4\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-poly-lr-decay==0.0.1) (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch-poly-lr-decay==0.0.1) (3.7.4.3)\n","Building wheels for collected packages: torch-poly-lr-decay\n","  Building wheel for torch-poly-lr-decay (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-poly-lr-decay: filename=torch_poly_lr_decay-0.0.1-cp37-none-any.whl size=3847 sha256=efa50c4b903c3391137b501b66313f6c78f2f31daf3cf2f65baa639ab9580baa\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cl8l2oas/wheels/5a/b7/09/d748b20c9bdfc768a33c37a28b2ad7dd9ec3e79e5152cb1618\n","Successfully built torch-poly-lr-decay\n","Installing collected packages: torch-poly-lr-decay\n","Successfully installed torch-poly-lr-decay-0.0.1\n"]}],"source":["# https://dacon.io/codeshare/2377?dtype=recent&s_id=0 코드 참조\n","!pip install git+https://github.com/cmpark0126/pytorch-polynomial-lr-decay.git"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1622736061177,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"hP6-9TCJg1_6"},"outputs":[],"source":["from torch_poly_lr_decay import PolynomialLRDecay\n","\n","decay_steps = (num_training//BATCH_SIZE)*EPOCHS\n","scheduler_poly_lr_decay = PolynomialLRDecay(optimizer, max_decay_steps=decay_steps, end_learning_rate=1e-6, power=0.9)"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1622736061593,"user":{"displayName":"DongHyoek Choi","photoUrl":"","userId":"12200645648842173499"},"user_tz":-540},"id":"8x3vO4sjFnGD"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import f1_score\n","import csv\n","\n","def cal_run_time(sec):\n","    times = str(datetime.timedelta(seconds=sec)).split(\".\")\n","    return times[0]\n","\n","\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n","    train_start = time.time()\n","\n","    #best_model_wts = copy.deepcopy(model.state_dict())\n","    best_f1_score = 0.0\n","    epoch_loss, epoch_score = [], []\n","    # Each epoch has a training and validation phase\n","    for epoch in range(1, num_epochs+1):\n","        print('***** Epoch {}/{} *****'.format(epoch, num_epochs))\n","        start = time.time()\n","\n","        ## Train\n","        model.train()\n","        train_loss_list = []\n","        epoch_training_loss_sum = 0.0\n","        for i_batch, sample_batched in enumerate(training_loader):\n","            b_recording = sample_batched[\"recording\"].to(device)       \n","            #b_sex=sample_batched[\"sex\"].to(device)\n","            #b_age=sample_batched[\"age\"].to(device)\n","            b_labels = sample_batched[\"labels\"].to(device)          \n","            optimizer.zero_grad()\n","            b_out = model(b_recording.float())                      \n","            loss = criterion(b_out, b_labels)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_training_loss_sum += loss.item() * b_labels.shape[0]\n","            scheduler.step()\n","        \n","        # batch train loss 기록 \n","        train_loss = epoch_training_loss_sum / num_training\n","        epoch_loss.append(train_loss)\n","\n","        # calculate time\n","        times = cal_run_time(time.time()-start)\n","        print('train_loss : {:.5f}\\ttime : {}\\n'.format(train_loss, times))\n","\n","        # epoch 5 단위마다 model 저장\n","        if epoch%5==0:\n","            torch.save(model,f'eff_model_{epoch}.pt')\n","\n","\n","        '''\n","        ## Validation\n","        model.eval()\n","        validation_prediction_df = pd.DataFrame(columns=['labels'])\n","        validation_prediction_df.index.name = 'id'\n","        validation_true_labels_df = pd.DataFrame(columns=['labels'])\n","        validation_true_labels_df.index.name = 'id'\n","\n","        with torch.no_grad():\n","            for idx in range(len(validation_set)):\n","                validation_sample = validation_set[idx]\n","                _, _, _, recording, labels = validation_sample  \n","                out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n","                sample_prediction = torch.nn.functional.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n","                indices_of_1s = np.where(sample_prediction.cpu())[0]\n","                str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n","                validation_prediction_df.loc[idx] = [str_indices_of_1s]\n","                        \n","                str_true_labels = ' '.join(labels)\n","                validation_true_labels_df.loc[idx] = [str_true_labels]\n","  \n","        mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n","        mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n","\n","        macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n","        epoch_score.append(macro_f1_validation)\n","\n","        # calculate time\n","        times = cal_run_time(time.time()-start())\n","\n","        print('train_loss : {:.5f}\\tvalid_f1_score : {:.5f}\\ttime : {}\\n'.format(train_loss, macro_f1_validation, times))\n","\n","\n","        # deep copy the model & save best model\n","        if macro_f1_validation > best_f1_score:\n","            best_idx = epoch\n","            best_f1_score = macro_f1_validation\n","            #best_model_wts = copy.deepcopy(model.state_dict())\n","            #torch.save(model.state_dict(), f'eff2_best_model.pt')\n","            torch.save(model, f'eff2_best_model.pt')\n","            print('==> best model saved - epoch: %d / f1_score: %.5f\\n'%(best_idx, best_f1_score))\n","        \n","\n","    times = cal_run_time(time.time()-train_start())\n","    print('Training complete in {}'.format(times))\n","    print('Best valid f1 score: %d - %.5f' %(best_idx, best_f1_score))\n","    '''\n","\n","    # calculate time\n","    times = cal_run_time(time.time()-train_start)\n","    print('Training completed in {}'.format(times))\n","\n","    # save epoch_loss & epoch_score\n","    with open('epoch_data', 'w', newline='') as f:\n","        writer = csv.writer(f)\n","        writer.writerow(epoch_loss)\n","        #writer.writerow(epoch_score)\n","    print('\\nepoch data saved.')\n","\n","    #return model, best_idx, best_f1_score, epoch_loss, epoch_score, validation_prediction_df, validation_true_labels_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TOPx23r2rrm","outputId":"937de276-622d-4342-daa9-904be9abfd54"},"outputs":[{"name":"stdout","output_type":"stream","text":["train_loss : 0.56028\ttime : 0:12:41\n","\n","***** Epoch 12/50 *****\n","train_loss : 0.54357\ttime : 0:12:45\n","\n","***** Epoch 13/50 *****\n","train_loss : 0.52626\ttime : 0:12:45\n","\n","***** Epoch 14/50 *****\n","train_loss : 0.51069\ttime : 0:12:45\n","\n","***** Epoch 15/50 *****\n","train_loss : 0.49538\ttime : 0:12:45\n","\n","***** Epoch 16/50 *****\n","train_loss : 0.48351\ttime : 0:12:44\n","\n","***** Epoch 17/50 *****\n","train_loss : 0.46373\ttime : 0:12:43\n","\n","***** Epoch 18/50 *****\n","train_loss : 0.44974\ttime : 0:12:43\n","\n","***** Epoch 19/50 *****\n","train_loss : 0.43043\ttime : 0:12:42\n","\n","***** Epoch 20/50 *****\n","train_loss : 0.41024\ttime : 0:12:43\n","\n","***** Epoch 21/50 *****\n","train_loss : 0.38939\ttime : 0:12:43\n","\n","***** Epoch 22/50 *****\n","train_loss : 0.37320\ttime : 0:12:42\n","\n","***** Epoch 23/50 *****\n","train_loss : 0.34953\ttime : 0:12:42\n","\n","***** Epoch 24/50 *****\n","train_loss : 0.32954\ttime : 0:12:42\n","\n","***** Epoch 25/50 *****\n","train_loss : 0.31232\ttime : 0:12:42\n","\n","***** Epoch 26/50 *****\n","train_loss : 0.29706\ttime : 0:12:42\n","\n","***** Epoch 27/50 *****\n"]}],"source":["# Train start\n","train_model(model, criterion, optimizer, scheduler_poly_lr_decay, EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"going-clerk","papermill":{"duration":0.026568,"end_time":"2021-05-14T11:36:19.520776","exception":false,"start_time":"2021-05-14T11:36:19.494208","status":"completed"},"tags":[]},"source":["### Evaluation\n","evalutate on validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AQWiPBBrvs7"},"outputs":[],"source":["'''\n","name=f'eff_best_model.pt'\n","model=torch.load(name)\n","model.to(device)\n","\n","model.eval()\n","\n","\n","validation_prediction_df = pd.DataFrame(columns=['labels'])\n","validation_prediction_df.index.name = 'id'\n","validation_true_labels_df = pd.DataFrame(columns=['labels'])\n","validation_true_labels_df.index.name = 'id'\n","\n","with torch.no_grad():\n","    for idx in range(len(validation_set)):\n","        validation_sample = validation_set[idx]\n","        _, _, _, recording, labels = validation_sample\n","        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n","        sample_prediction = torch.nn.functional.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n","        indices_of_1s = np.where(sample_prediction.cpu())[0]\n","        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n","        validation_prediction_df.loc[idx] = [str_indices_of_1s]\n","        \n","        str_true_labels = ' '.join(labels)\n","        validation_true_labels_df.loc[idx] = [str_true_labels]\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:36:48.988024Z","iopub.status.busy":"2021-05-14T11:36:48.987356Z","iopub.status.idle":"2021-05-14T11:36:48.997122Z","shell.execute_reply":"2021-05-14T11:36:48.996588Z"},"id":"indirect-gates","papermill":{"duration":0.043191,"end_time":"2021-05-14T11:36:48.997257","exception":false,"start_time":"2021-05-14T11:36:48.954066","status":"completed"},"tags":[]},"outputs":[],"source":["#print(validation_prediction_df[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:36:49.059764Z","iopub.status.busy":"2021-05-14T11:36:49.059155Z","iopub.status.idle":"2021-05-14T11:36:49.062331Z","shell.execute_reply":"2021-05-14T11:36:49.063058Z"},"id":"unlimited-building","papermill":{"duration":0.037905,"end_time":"2021-05-14T11:36:49.063276","exception":false,"start_time":"2021-05-14T11:36:49.025371","status":"completed"},"tags":[]},"outputs":[],"source":["#print(validation_true_labels_df[:10])"]},{"cell_type":"markdown","metadata":{"id":"recognized-windsor","papermill":{"duration":0.028538,"end_time":"2021-05-14T11:36:50.416631","exception":false,"start_time":"2021-05-14T11:36:50.388093","status":"completed"},"tags":[]},"source":["### Test Prediction\n","학습된 모델로 test_set에 대한 prediction을 진행합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:38:38.301246Z","iopub.status.busy":"2021-05-14T11:38:38.300546Z","iopub.status.idle":"2021-05-14T11:39:17.295789Z","shell.execute_reply":"2021-05-14T11:39:17.296466Z"},"id":"wicked-necessity","papermill":{"duration":39.0374,"end_time":"2021-05-14T11:39:17.296645","exception":false,"start_time":"2021-05-14T11:38:38.259245","status":"completed"},"tags":[]},"outputs":[],"source":["model.eval()\n","\n","test_prediction_df = pd.DataFrame(columns=['labels'])\n","test_prediction_df.index.name = 'id'\n","\n","with torch.no_grad():\n","    for idx in range(len(test_set)):\n","        test_sample = test_set[idx]\n","        _, _, _, recording = test_sample\n","        out = model(torch.tensor(recording).unsqueeze(0).to(device)) # unsqueeze는 batch dimension을 추가해주기 위함\n","        sample_prediction = torch.nn.functional.sigmoid(out).squeeze() > 0.5 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n","        indices_of_1s = np.where(sample_prediction.cpu())[0]\n","        str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n","        test_prediction_df.loc[idx] = [str_indices_of_1s]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:39:17.359758Z","iopub.status.busy":"2021-05-14T11:39:17.358870Z","iopub.status.idle":"2021-05-14T11:39:17.374238Z","shell.execute_reply":"2021-05-14T11:39:17.374734Z"},"id":"unavailable-nursery","papermill":{"duration":0.048352,"end_time":"2021-05-14T11:39:17.374919","exception":false,"start_time":"2021-05-14T11:39:17.326567","status":"completed"},"tags":[]},"outputs":[],"source":["test_prediction_df[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-05-14T11:39:17.440968Z","iopub.status.busy":"2021-05-14T11:39:17.440006Z","iopub.status.idle":"2021-05-14T11:39:17.465484Z","shell.execute_reply":"2021-05-14T11:39:17.464990Z"},"id":"victorian-coaching","papermill":{"duration":0.059779,"end_time":"2021-05-14T11:39:17.465630","exception":false,"start_time":"2021-05-14T11:39:17.405851","status":"completed"},"tags":[]},"outputs":[],"source":["test_prediction_df.to_csv('my_submission.csv')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["m7vedTABfzR_","rubber-missile","broad-victoria","delayed-nowhere","going-clerk"],"machine_shape":"hm","name":"eff_model_final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.5 ('pythonenv')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.5"},"vscode":{"interpreter":{"hash":"9958a56ca5485943583299b4d3627347dacf3a53366ef83dbbf1d65f5917f338"}}},"nbformat":4,"nbformat_minor":0}
